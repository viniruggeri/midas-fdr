# **MIDAS FDR 2 â€” Financial Deep Reasoning**

> *Beyond retrieval. Persistent inferential reasoning.*

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-red.svg)](https://pytorch.org/)
[![Neo4j](https://img.shields.io/badge/Neo4j-5.13-green.svg)](https://neo4j.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**ğŸ“„ [Whitepaper](docs/whitepaper-fdr.md) | ğŸš€ [Quick Start](docs/QUICK_START.md) | ğŸ“Š [MVP PoC](docs/MVP_PROOF_OF_CONCEPT.md) | ğŸ“ [Changelog](docs/CHANGELOG.md) | ğŸ“ [Project Structure](PROJECT_STRUCTURE.md)**

---

## ğŸ§  **Overview**

**MIDAS FDR 2 (Financial Deep Reasoning)** is a neuroelastic cognitive framework for persistent inferential reasoning based on dynamic topological graphs.

Unlike traditional RAG (Retrieval-Augmented Generation) systems that rely on static embeddings, **FDR 2 maintains neuroelastic contextual persistence** â€” allowing reasoning to evolve without loss of semantic coherence.

The system introduces a **Deep Reasoning Layer (DRL)** capable of:
- ğŸ”— **Persistent context** through graph topology
- ğŸ§¬ **Neuroelastic adaptation** inspired by biological neuroplasticity
- ğŸŒŠ **Multi-hop reasoning** with GNN-enhanced inference
- ğŸ”„ **Self-healing** via Aphelion Layer (extinction/rebirth cycles)
- ğŸ’­ **What-if scenarios** with financial simulation

**Key Innovation:** The system does not summarize â€” it **thinks inferentially**.

### Academic Context

**Project:** FIAP (AnÃ¡lise e Desenvolvimento de Sistemas) â€” Sprint 2  
**Author:** VinÃ­cius Ruggeri  
**Date:** November 2025  
**Version:** 2.0.0 (FDR v2 - Deep Reasoning Layer)

> **Note:** This documentation describes **FDR v2** with neuroelastic reasoning and GNN inference. For v1 (basic RAG), see [CHANGELOG.md](docs/CHANGELOG.md).

---

## ğŸš€ **Quick Start**

### Prerequisites
- Docker Desktop (for Neo4j)
- Python 3.10+
- 8GB RAM minimum

### Automated Setup (Windows)
```powershell
.\scripts\setup.ps1
```

### Manual Setup
```bash
# 1. Start Neo4j
docker-compose up -d

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure environment
cp .env.example .env
# Edit .env with your OPENAI_API_KEY

# 4. Start server
python -m uvicorn app.main:app --reload --port 8000

# 5. Populate graph
curl -X POST http://localhost:8000/graph/populate

# 6. Train GNN
python scripts/train_gnn.py

# 7. Test system
python scripts/demo_mvp.py
```

**Full guide:** [docs/QUICK_START.md](docs/QUICK_START.md)

---

## ğŸ”¬ **Core Concepts**

### 1. Neuroelastic Reasoning
Inspired by biological neuroplasticity, FDR 2 maintains **adaptive connections** between contexts:
- Connections **expand**, **retract**, or **reconfigure** based on semantic flow
- New contexts don't overwrite â€” they **realign** through elastic re-weighting
- Preserves **persistent meaning** while remaining **adaptively plastic**

### 2. Persistent Inferential Graphs
Each node = tokenized context | Each edge = active inference

```
Traditional RAG: Query â†’ Embed â†’ Retrieve â†’ Generate â†’ Forget
FDR 2:          Query â†’ Graph â†’ Reason â†’ Generate â†’ Persist
```

### 3. Aphelion Layer (Semantic Survival)
When global coherence collapses, the system undergoes **controlled extinction and rebirth**:
- Extract core concepts via PageRank
- Prune low-relevance nodes
- Reconstruct graph from latent backups

### 4. GNN-Enhanced Multi-hop Reasoning
Graph Attention Networks (GAT) provide **topological awareness**:
- Node relevance scoring
- Multi-hop path discovery
- Confidence calibration based on graph structure

### 5. raRg Paradigm
**Retrieval-Augmented Reasoning Generation** (not just retrieval):
- GPT/LLM = Semantic interpreter
- FDR Graph = Contextual memory substrate
- Reasoning emerges from **topology**, not just embeddings

---

## âš™ï¸ **Architecture Overview (v2)**

### **High-Level Architecture**

```mermaid
graph TB
    subgraph User Layer
        A[Natural Language Query]
    end
    
    subgraph Deep Reasoning Layer DRL
        B[Intent Classifier]
        B --> C[Multi-hop Reasoner]
        C --> D[Neuroelastic Graph]
        D --> E[GNN Inference GAT]
        E --> F[Aphelion Layer]
        F --> G[Entropy Regulator]
    end
    
    subgraph Knowledge Layer
        H[(Neo4j Graph)]
        I[SentenceTransformer]
        J[PyTorch Geometric]
    end
    
    subgraph Generation Layer
        K[GPT-4 / LLM]
        L[HumanizerLLM]
        M[ICE Context]
    end
    
    A --> B
    D <--> H
    E <--> J
    G --> M
    M --> K
    K --> L
    L --> N[Humanized Response]
    
    style D fill:#BD10E0
    style E fill:#4A90E2
    style F fill:#F5A623
    style L fill:#7ED321
```

### **Query Processing Flow**

```mermaid
sequenceDiagram
    participant U as User
    participant IC as Intent Classifier
    participant MH as Multi-hop Reasoner
    participant GNN as GNN Inference
    participant APH as Aphelion Layer
    participant LLM as HumanizerLLM
    
    U->>IC: "If I stop ordering gnocchi, how close to my travel goal?"
    IC->>IC: Classify: what-if scenario
    IC->>MH: Extract entities: ["gnocchi", "travel goal"]
    
    MH->>GNN: Query neuroelastic graph
    GNN->>GNN: GAT inference (node relevance)
    GNN-->>MH: Top nodes + confidence scores
    
    MH->>MH: Multi-hop traversal (depth=3)
    MH-->>APH: Check coherence (C=0.82)
    APH-->>MH: âœ“ Above threshold (Ï„=0.70)
    
    MH->>LLM: Assemble ICE context
    LLM->>LLM: Generate humanized response
    LLM-->>U: "VocÃª economiza R$120/mÃªs..."
```

### **Core Components**

| Component | Technology | Function |
|-----------|-----------|----------|
| **Neuroelastic Graph** | Neo4j + NetworkX | Dynamic topology with persistent context |
| **GNN Inference** | PyTorch Geometric (GAT) | Node relevance + confidence prediction |
| **Aphelion Layer** | PageRank + Pruning | Semantic survival and reconstruction |
| **Multi-hop Reasoner** | Python + NumPy | Iterative depth-first search with ICE assembly |
| **HumanizerLLM** | GPT-4 / Claude | Natural language generation |

---

## ğŸ”„ **Pipeline de Dados**

O fluxo de dados segue o modelo **Event-Driven Sync**, no qual eventos de novas transaÃ§Ãµes ou atualizaÃ§Ãµes no Oracle disparam rotinas de ingestÃ£o para as demais camadas.

### **Diagrama de IngestÃ£o**

```mermaid
flowchart LR
    A[Oracle<br/>Transaction Created] -->|Event| B[RabbitMQ Queue]
    B --> C[Python Sync Worker]
    
    C --> D[NER Extractor]
    C --> E[Embedding Generator]
    C --> F[Graph Builder]
    
    D -->|Entities| G[(PostgreSQL<br/>NER Store)]
    E -->|Vectors| H[(PostgreSQL<br/>pgvector)]
    E -->|Index| I[FAISS Cache]
    F -->|Relations| J[(Neo4j)]
    
    G -.->|Feeds| K[GFQR Training]
    J -.->|Feeds| K
    
    style A fill:#F5A623
    style C fill:#4A90E2
    style K fill:#BD10E0
```

### **Etapas de Processamento**

1. **IngestÃ£o de dados** do Oracle (transaÃ§Ãµes, categorias, metadados).
2. **ExtraÃ§Ã£o de entidades** via modelo NER financeiro (ex: "Uber", "Spotify", "delivery").
3. **GeraÃ§Ã£o de embeddings** com vetorizaÃ§Ã£o contextual.
4. **AtualizaÃ§Ã£o do grafo** no Neo4j, representando relaÃ§Ãµes e padrÃµes emergentes.
5. **DisponibilizaÃ§Ã£o dos dados** para os mÃ³dulos de *retrieval* do FDR.

---

## ğŸ§© **Mecanismo FDR**

O motor FDR Ã© composto por trÃªs *retrievers* independentes e cooperativos:

| Retriever                                        | Tipo                       | FunÃ§Ã£o Principal                                        |
| ------------------------------------------------ | -------------------------- | ------------------------------------------------------- |
| **Graph RAG**                                    | Consultas Neo4j            | Detecta tendÃªncias, relaÃ§Ãµes e padrÃµes temporais.       |
| **Vectorial RAG**                                | Similaridade vetorial      | Localiza transaÃ§Ãµes semanticamente prÃ³ximas.            |
| **GFQR** (*Generative Financial Query Reasoner*) | GNN + RaciocÃ­nio simbÃ³lico | Executa anÃ¡lises hipotÃ©ticas e inferÃªncias financeiras. |

Esses mÃ³dulos sÃ£o coordenados por um **Router de LangGraph**, que define dinamicamente o plano de consulta (`retriever_plan`) com base na intenÃ§Ã£o e complexidade da query.

### **Arquitetura GFQR**

```mermaid
graph TD
    A[Query] --> B[Query Encoder]
    B --> C[Subgraph Extractor]
    
    C --> D[(Neo4j)]
    D --> E[Financial Subgraph]
    
    E --> F[GNN Encoder]
    F --> G[Node Embeddings]
    F --> H[Edge Embeddings]
    
    B --> I[Query Attention Layer]
    G --> I
    H --> I
    
    I --> J[Reasoning MLP]
    J --> K[Multi-hop Inference]
    J --> L[Numeric Computation]
    
    K --> M[Answer Decoder]
    L --> M
    
    M --> N[Final Answer + Reasoning Path]
    
    style F fill:#BD10E0
    style J fill:#7ED321
    style M fill:#4A90E2
```

---

## ğŸ“Š **Exemplo de RaciocÃ­nio FDR**

**Query:**

> â€œPor que meus gastos com delivery aumentaram 40% nos Ãºltimos 3 meses e quanto eu economizaria se pedisse sÃ³ aos finais de semana?â€

### **Etapas do Pipeline**

* **NER:** identifica categorias (â€œdeliveryâ€), perÃ­odos (â€œÃºltimos 3 mesesâ€) e aÃ§Ã£o (â€œeconomizariaâ€).
* **Intent Classification:** define o tipo de anÃ¡lise: `trend_analysis + what_if_scenario`.
* **Retriever Plan:** seleciona `["graph", "vectorial", "gfqr"]`.
* **Graph RAG:** confirma aumento de 40% nos Ãºltimos 3 meses.
* **Vectorial RAG:** identifica padrÃ£o temporal (60% dos pedidos durante a semana).
* **GFQR:** estima economia hipotÃ©tica em caso de restriÃ§Ã£o a finais de semana.

---

## ğŸ” **Explainability Example**

**Resposta Final:**

> â€œSeus gastos com delivery aumentaram de R$180 (julho) para R$252 (setembro), um crescimento de 40%.
> O padrÃ£o mostra que 60% dos pedidos ocorrem durante a semana.
> Se vocÃª restringir pedidos apenas aos finais de semana, economizarÃ¡ aproximadamente R$128/mÃªs (R$1.536/ano).â€

**TraÃ§o de RaciocÃ­nio (Reasoning Trace):**

* *Graph RAG â†’ Trend Analysis*
* *Vectorial RAG â†’ Pattern Detection*
* *GFQR â†’ Hypothetical Reasoning & Simulation*

**Caminho de RecuperaÃ§Ã£o (Retriever Path):**

```
graph â†’ vectorial â†’ gfqr
```

**ConfianÃ§a:** 0.91
**LatÃªncia MÃ©dia:** ~2.8s

---

## ğŸ§© **FDR v2 vs Traditional RAG**

| Capability | RAG v1 | FDR v2 | Notes |
|-----------|--------|--------|-------|
| Simple queries | âœ… | âœ… | Both handle basic lookup |
| Pattern detection | âŒ | âœ… | Requires graph topology |
| Multi-hop reasoning | âŒ | âœ… | GNN-guided traversal |
| What-if scenarios | âŒ | âœ… | Graph simulation |
| Context persistence | âŒ | âœ… | Neuroelastic memory |
| Self-healing | âŒ | âœ… | Aphelion extinction |
| Semantic coherence | âŒ | âœ… | C(G) monitoring |
| Explainability | âš ï¸ | âœ… | Full reasoning trace |

**Key Innovation:** FDR v2 doesn't just retrieve â€” it **reasons persistently** across a living knowledge graph.

---

## ğŸ“˜ **Resumo Conceitual**

FDR 2 represents the transition from **context compression** to **context evolution** â€” a shift toward synthetic intelligence where reasoning itself becomes a dynamic, adaptive structure.

> "Enquanto o RAG responde, o FDR pensa."

---

## ğŸ“Š **Project Stats**

- **Code:** ~2,687 lines of Python
- **Architecture:** 4 core cognitive modules
- **Documentation:** 5 comprehensive guides
- **Training:** 15 epochs, ~2-3 min CPU
- **Model:** GAT-based GNN, ~50k parameters, 200KB
- **Performance:** 100-1000ms per query
- **Coverage:** Simple queries, pattern detection, what-if scenarios

**Technology Stack:**
```
FastAPI + Neo4j + PyTorch Geometric + SentenceTransformers + LangChain
```

---

## ğŸ“ **Academic Impact**

This project demonstrates:
- âœ… Advanced system architecture (multi-layer, distributed)
- âœ… Custom ML implementation (GNN from scratch)
- âœ… Novel theoretical framework (neuroelasticity, Aphelion Layer)
- âœ… Production-ready engineering (Docker, async, error handling)
- âœ… Comprehensive documentation (whitepaper, guides, changelog)

**Sprint 2 Deliverable:** âœ… Complete MVP with demonstrable AI/ML reasoning

---

## ğŸ“¬ **Contact**

**Author:** VinÃ­cius Ruggeri  
**GitHub:** [@viniruggeri](https://github.com/viniruggeri)  
**Project:** [midas-fdr](https://github.com/viniruggeri/midas-fdr)  
**Institution:** FIAP â€” AnÃ¡lise e Desenvolvimento de Sistemas

---

## ğŸ“„ **License**

MIT License â€” See [LICENSE](LICENSE) for details

---

**Built with â¤ï¸ for the future of cognitive architectures**
